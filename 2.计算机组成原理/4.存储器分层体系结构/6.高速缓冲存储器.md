好的，这是关于“4.6 高速缓冲存储器”部分的学习笔记：

**4.6 高速缓冲存储器 (Cache)**

在本章4.5节中提到，通过提高存储芯片本身的速度或采用并行存储器结构可以缓解CPU和主存之间的速度匹配问题。除了这两种方法以外，在CPU和主存之间设置高速缓存 (cache) 也可以提高CPU访问指令和数据的速度。

**4.6.1 程序访问的局部性 (Locality of Reference)**

对大量典型程序运行情况分析的结果表明，在较短时间间隔内，程序产生的地址往往集中在存储器的一个很小范围。这种现象称为程序访问的局部性，可细分为时间局部性和空间局部性。

*   **时间局部性 (Temporal Locality)**：指被访问的某个存储单元在一个较短的时间间隔内很可能又被访问。
    *   原因：程序中循环结构、子程序调用等会导致指令的重复执行。
*   **空间局部性 (Spatial Locality)**：指被访问的某个存储单元的邻近单元在一个较短的时间间隔内很可能也被访问。
    *   原因：指令通常是顺序存放的，数据（如数组元素）也常常是连续存放的。

**示例 (高级语言程序段及其汇编表示)**：
```c
// 高级语言
1 sum=0;
2 for (i=0; i<n; i++)
3   sum+=a[i];
4 *v=sum;
// 中间语言描述 (示意)
I0  sum = 0
I1  ap = A           ; A是数组的起始地址
I2  i = 0
I3  if (i>=n) goto done
I4 loop: t = *(ap)       ; 数组元素 a[i] 的值
I5  sum = sum + t    ; 累加值在 sum 中
I6  ap = ap + 4      ; 计算下一个数组元素的地址
I7  i = i + 1
I8  if (i<n) goto loop
I9 done: V = sum          ; 累加结果保存至地址 V
```
* ![[image-167.png]]  
* **指令访问局部性**：
    *   顺序执行：I0~I3顺序执行（空间局部性）。
    *   循环执行：I4~I8循环执行n次（时间局部性和空间局部性）。
*   **数据访问局部性**：
    *   数组a在主存中连续存放，指令I4按顺序访问数组元素（空间局部性）。
*   **利用空间局部性**：通常把当前访问单元以及邻近单元作为一个主存块一起调入cache。主存块的大小以及程序对数组元素的访问顺序等都会对程序性能产生影响。
  
**例4.2：数组访问的局部性影响**
比较按行优先和按列优先访问二维数组 `a[M][N]` 时的空间局部性
```c
//程序段A
int sum-array-rows (int a[M][N])
{
    int i, j, sum=0;
    for (i=0; i<M; i++)
        for (j=0; j<N; j++)
            sum+=a[i][j];
    return sum;

```
```c
//程序段B
int sum-array-cols (int a[M][N])
{
    int i, j, sum=0;
    for (j=0; j<N; j++)
        for (i=0; i<M; i++)
            sum+=a[i][j];
    return sum;
}
```
![[image-168.png]]
*   **程序段A (按行访问)**：`for i ... for j ... sum += a[i][j]`
    *   访问顺序与存储顺序一致（假设按行存储），空间局部性好。
*   **程序段B (按列访问)**：`for j ... for i ... sum += a[i][j]`
    *   访问顺序与存储顺序不一致，每次访问下一个元素需要跳过N个元素。如果cache块大小小于N个元素，则每次装入cache块后，下一个要访问的元素很可能不在该块中，导致空间局部性差。
*   **变量 `sum` 的局部性**：
    *   空间局部性：对单个变量无意义。
    *   时间局部性：在A和B中都较好，因为`sum`在每次循环中都被访问。通常编译器会将其分配在寄存器中。
*   **`for` 循环体的局部性**：
    *   空间局部性：好，循环体内指令按序连续存放。
    *   时间局部性：好，内循环体被连续重复执行多次。
*   **性能差异**：程序A通常比程序B快很多，尤其当M、N很大时。

**4.6.2 cache 的基本工作原理**

*   **定义**：cache是一种小容量高速缓冲存储器，由快速的SRAM组成，直接制作在CPU芯片内，速度几乎与CPU一样快。
*   **作用**：在CPU和主存之间设置cache，总是把主存中被频繁访问的活跃程序块和数据块复制到cache中。
*   **原理**：利用程序访问的局部性，大多数情况下，CPU能直接从cache中取得指令和数据，而不必访问主存。
*   **信息交换单位**：
    *   **主存中的区域**：称为块 (block) 或主存块，是cache和主存之间的信息交换单位。
    *   **cache中存放主存块的区域**：称为行 (line) 或槽 (slot)。
*   **cache的映像**：cache中的内容是主存中部分内容的副本。

1.  **cache的有效位 (Valid Bit)**
    *   **作用**：在系统启动或复位时，每个cache行都为空，其中的信息无效。只有装入了主存块后信息才有效。为了说明cache行中的信息是否有效，每个cache行需要一个“有效位”。
    *   **冲刷 (flush)**：通过将有效位清0来淘汰某cache行中的主存块。装入新主存块时，再使有效位置1。

2.  **CPU在cache中的访问过程 (图4.22)**
	   ![[image-169.png]]
    1.  **检查cache**：CPU执行程序时，需要从主存取指令或读数据时，先检查cache中是否有要访问的信息。
    2.  **cache命中 (Hit)**：若有，则直接从cache中读取，不访问主存。
    3.  **cache缺失 (Miss) / 未命中**：若没有，则再从主存中把当前访问信息所在的一个主存块复制到cache中。
    4.  **cache替换**：如果cache中对应的行已被占用且需要替换，则执行替换算法。
    *   **硬件实现**：整个访存过程，包括判断、取信息、替换等，都由硬件自动完成，对程序员透明。

3.  **cache-主存层次的平均访问时间 (Ta)**
    *   **命中率 (Hit Rate, p)**：CPU访问单元所在的块在cache中的概率 (等于命中次数 / 访问总次数)。
    *   **缺失率 (Miss Rate, 1-p)**：CPU访问单元所在的块不在cache中的概率 (等于不命中次数 / 访问总次数)。
    *   **命中时间 (Hit Time, Tc)**：CPU在cache中直接存取信息所用的时间开销 (即cache访问时间)。
    *   **缺失代价 (Miss Penalty, Tm_penalty)**：缺失时，从主存读取一个主存块到cache的时间。通常指主存访问时间Tm。
    *   **平均访问时间公式**：
        `Ta = p * Tc + (1-p) * (Tm + Tc)`  (更精确的，如果缺失时也算上一次Tc)
        或者简化为： `Ta = Tc + (1-p) * Tm` (假设缺失代价Tm已包含Tc，或者Tc相对Tm很小)
    *   **特点**：由于程序访问的局部性，cache的命中率可以达到很高 (接近1)。因此，虽然缺失代价远大于命中时间，但最终的平均访问时间仍可接近cache的访问时间。

**例4.3：cache命中率与平均访问时间计算**
	 假定处理器时钟周期为 2ns, 某程序有 1000 条指令组成, 每条指令执行一次, 其中的 4 条指令在取指令时, 没有在 cache 中找到, 其余指令都能在 cache 中取到。在执行指令过程中, 该程序需要 3000 次主存数据访问, 其中, 6 次没有在 cache 中找到。试回答以下问题。
		(1) 执行该程序得到的 cache 命中率是多少?
	     (2) 若 cache 中存取一个信息的时间为一个时钟周期, 缺失损失为 4 个时钟周期, 则 CPU 在 cache-主存层次的平均访问时间为多少?
			解析:  (1) 计算cache命中率：根据命中次数和总访问次数计算。
				  (2) 计算平均访问时间：代入公式 `Ta = Tc + (1-p) * Tm_penalty`
	  
```
(1) 执行该程序时的总访问次数为 1000+3000 = 4000，未命中次数为 4+6 = 10。  
cache 命中率为 (4000-10)/4000 = 99.75%。  
(2) cache-主存层次的平均访问时间为 1+(1-99.75%)×4 = 1.01 个时钟周期, 相当于 1.01×2ns = 2.02ns, 与 cache 的访问时间相近
```

**4.6.3 cache行和主存块之间的映射方式**

cache行中的信息取自主存中的某个块。在将主存块复制到cache行时，主存块和cache行之间必须遵循一定的映射规则，这样CPU要访问某个主存单元时，可以依据映射规则到cache对应的行中查找信息，而不用在整个cache中查找。

1.  **直接映射 (Direct Mapping)**
    *   **基本思想**：把主存的每一块映射到固定的一个cache行中。也称模映射。
    *   **映射关系**：`cache行号 = 主存块号 mod cache总行数`
    *   **地址划分**：主存地址被分为三个字段：
	    * ![[image-171.png]]
        *   **标记 (Tag)**：主存块号的高位部分，用于区分映射到同一cache行的不同主存块。
        *   **cache行号 (Index/Set Index)**：主存块号的低位部分，直接指定映射到哪个cache行。
        *   **块内地址 (Offset)**：用于在块内寻址。
    *   **CPU访存过程 (图4.23)**：
	    * ![[image-170.png]]
        1.  根据访存地址中间的“cache行号”字段，直接找到对应的cache行。
        2.  比较该cache行中的“标记”和访存地址的高位“标记”字段。
        3.  若标记相等且有效位为1，则cache命中，根据“块内地址”字段存取信息。
        4.  若标记不相等或有效位为0，则cache缺失，CPU从主存读出该地址所在的一块信息到对应的cache行，更新标记和有效位，并将所需信息送CPU。
    *   **写操作**：比读操作复杂，涉及cache与主存数据一致性问题 (将在4.6.5节讨论)。
    *   **优点**：实现简单，命中时间短（查找快）。
    *   **缺点**：不灵活，如果程序交替访问多个映射到同一cache行的主存块（称为“冲突”或“抖动”），即使其他cache行空闲，也会频繁发生替换，导致命中率低。

**例4.4：直接映射地址划分与访问过程**
	    假定主存和 cache 之间采用直接映射方式, 块大小为 512 字。cache 容量 (指数据区) 为 8K 字, 主存地址空间为 1M 字。问: 主存地址如何划分? 用图表示主存块和 cache 行之间的映射关系, 假定 cache 当前为空, 说明 CPU 对主存单元 0240CH 的访问过程
	    根据给定的cache容量、主存容量、块大小，计算标记、行号、块内地址的位数，并描述访存过程。
        **解**:cache 数据区容量为 8K 字 = 2¹³ 字 = 2⁴ 行 × 512 字/行 = 16 行 × 512 字/行。  
		因为主存每 16 块和 cache 的 16 行一一对应, 所以可将主存每 16 块看成一个块群。主存地址空间为 1M 字 = 2²⁰ 字 = 2¹¹ 块 × 512 字/块 = 2⁷ 块群 × 2⁴ 块/块群 × 512 字/块。所以, 主存地址位数 n=20, 标记位数 t=7, 行号位数 c=4, 块内地址位数为 9。
		![[image-172.png]]
**例4.5：块大小对直接映射的影响**
	假定主存和 cache 之间采用直接映射方式, 块大小为一个字节。cache 容量 (指数据区) 为 4 个字节, 主存地址为 32 位, 按字节编址。问: 主存地址如何划分? 根据程序访问的局部性原理说明块大小设置为 1 个字节时的缺陷
	  **解:** 
	  块大小为 1 个字节, 故块内无须寻址, 块内地址位数为 0。cache 的数据区存放 4 个字节, 共有 4 个行。因此, 主存地址位数 n=32, 被划分为两个字段: 标记位数 t=30, 行号位数 c=2
	  块太小产生的问题（如1字节）：
          邻近单元的访问可能发生缺失，没有利用好空间局部性。
          标记字段会很长，增加了标记存储开销。
          在cache行数不变的情况下，映射到同一cache行的主存块数增加，冲突概率增大。
**例4.6：直接映射cache设计与总容量计算**
	假定主存和 cache 之间采用直接映射方式, 块大小为 16B。cache 的数据区容量为 64KB, 主存地址为 32 位, 按字节编址。问: 主存地址如何划分? 说明访存过程, 并计算 cache 总容量为多少?
    根据给定的参数计算地址划分，并计算cache的总容量（包括数据区和标记区、有效位等）。
     **解答**:cache 数据区容量为 64KB = 2¹⁶B = 2¹² 行 × 2⁴B/行。  
		因为主存的每 2¹² 块和 cache 的 2¹² 行一一对应, 所以可将主存的每 2¹² 块看成一个块群, 因而, 得到主存空间划分为 2³²B = 2²⁸ 块 × 2⁴B/块 = 2¹⁶ 块群 × 2¹² 块/块群 × 2⁴B/块。  
		因此, 主存地址位数 n=32, 其中标记位数 t=16, 行号位数 c=12, 块内地址位数为 4。![[image-173.png]]
		**访存过程**:① :根据 12 位 cache 行索引找到对应行; ②: 将 16 位标记与对应行中的标记比较; ③ :比较相等并有效位为 1 时, 输出“Hit”为 1; ④: 由两位字偏移量从 4 个 32 位字中选择一个字输出; ⑤ :由两位字节偏移量从一个 32 位字中选择一个字节输出。CPU 在“Hit”为 1 的情况下, 根据要访问的是字还是字节选择从第 ④ 步还是第 ⑤ 步得到结果。若“Hit”不为 1, 则 CPU 要启动一次“cache 行读”总线事务操作, 通过总线到主存读一块连续的信息到 cache 行中。
2.  **全相联映射 (Fully Associative Mapping)**
    *   **基本思想**：一个主存块可以装入cache的任意一行中。
    *   **地址划分**：主存地址被分为两个字段：
        *   **标记 (Tag)**：主存块号。
        *   **块内地址 (Offset)**。
    *   **CPU访存过程**：
        1.  将访存地址的“标记”字段与cache中所有行的“标记”同时进行比较。
        2.  若找到一个标记相等且有效位为1的行，则cache命中，根据“块内地址”存取信息。
        3.  若所有行都不匹配或有效位为0，则cache缺失，从主存读入主存块到cache的任一空闲行（或根据替换算法选择一行）。
    *   **实现**：通常每个cache行都设置一个比较器，是一种“按内容访问”的相联存储器。
    *   **优点**：灵活，块冲突概率低（只有当cache全满时才会发生替换）。
    *   **缺点**：比较电路复杂，成本高，比较速度慢（尤其当cache容量大时），不适合容量较大的cache。

**例4.7：全相联映射地址划分与访问过程**
	假定主存和 cache 之间采用全相联映射, 块大小为 512 字。cache 容量 (指数据区) 为 8K 字, 主存地址空间为 1M 字。问: 主存地址如何划分? 用图表示主存块和 cache 行之间的映射关系, 并说明 CPU 对主存单元 0240CH 的访问过程
    根据参数计算地址划分，并描述访存过程。
      ![[image-174.png]]
      **访问过程**:访问 0240CH 单元的过程为: 首先将高 11 位标记 0000 0010 010 与 cache 中每个行的标记进行比较, 若有一个相等并且对应有效位为 1, 则命中, 此时 CPU 根据块内地址 0 0000 1100 从该行中取出信息; 若都不相等, 则不命中, 此时, 需要将 0240CH 单元所在的主存第 0000 0010 010 块 (即第 18 块) 复制到 cache 的任何一个空闲行中, 并置有效位为 1, 置标记为 0000 0010 010 (表示信息取自主存第 18 块)。
     **总结** :为了加快比较的速度, 通常每个 cache 行都设置一个比较器, 比较器位数等于标记字段的位数。全相联 cache 访存时根据标记内容来访问 cache 行中的主存块, 因而它查找主存块的过程是一种“按内容访问”的存取方式, 因此, 它是一种“相联存储器”。相联映射方式的时间开销和所用元件开销都较大, 实现起来比较困难, 不适合容量较大的 cache

3.  **组相联映射 (Set Associative Mapping)**
    *   **基本思想**：是直接映射和全相联映射的折中。将cache的所有行分成若干组 (Set)，主存块映射到cache的固定组中的任意一行。组间采用直接映射，组内采用全相联映射。
    *   **映射关系**：`cache组号 = 主存块号 mod cache总组数`
    *   **N路组相联 (N-way Set Associative)**：每组包含N行。
        *   N=1时，即为直接映射。
        *   若cache只有一组 (组内行数 = cache总行数)，即为全相联映射。
    *   **地址划分**：主存地址被分为三个字段：
	    * ![[image-177.png]]
        *   **标记 (Tag)**：主存块号的高位部分。
        *   **组号 (Set Index)**：用于确定映射到哪个cache组。
        *   **块内地址 (Offset)**。
    *   **CPU访存过程 (图4.27，以2路组相联为例)**：
	    * ![[image-175.png]]
        1.  根据访存地址中间的“组号”字段，直接找到对应的cache组。
        2.  将该组内所有行的“标记”与访存地址的高位“标记”字段同时进行比较。
        3.  若组内某行标记相等且有效位为1，则cache命中，根据“块内地址”存取信息。
        4.  若组内所有行都不匹配或有效位为0，则cache缺失，从主存读入主存块到该组的任一空闲行（或根据替换算法选择一行）。
    *   **参数s**：如果每组有 `2^s` 行，则称为 `2^s` 路组相联。s的选取影响冲突概率和比较复杂度。
    *   **优点**：块冲突概率较直接映射低，比较电路比全相联简单，查找速度比全相联快。

**例4.8：组相联映射地址划分与访问过程**
	假定主存和 cache 之间采用 2 路组相联映射, 块大小为 512 字。cache 容量 (指数据区) 为 8K 字, 主存地址空间为 1M 字。问: 主存地址如何划分? 用图表示主存块和 cache 行之间的映射关系, 并说明 CPU 对主存单元 0240CH 的访问过程
    根据参数计算地址划分，并描述访存过程。
      **解答**:cache 数据区容量为 8K 字 = 2¹³ 字 = 2³ 组 × 2¹ 行/组 × 512 字/行。  
      主存地址空间为 1M 字 = 2²⁰ 字 = 2¹¹ 块 × 512 字/块 = 2⁸ 组群 × 2³ 块/组群 × 512 字/块。  
      所以, 主存地址位数 n=20, 标记位数 m=8, 组号位数 q=3, 块内地址位数为 9。  
      主存地址划分以及主存块和 cache 行的对应关系如图 4.28 所示。（注：图中地址从高位到低位依次是标记、cache 组号、块内地址，位宽分别是 8、3、9）
      ![[image-178.png]]
      **访存过程**: 首先根据地址中间 3 位 010, 找到 cache 第 2 组, 将标记 0000 0010 与第 2 组中两个 cache 行的标记同时进行比较, 若有一个相等并且有效位是 1, 则命中。此时根据低 9 位块内地址从对应行中取出单元内容送 CPU; 若都不相等或有一个相等但有效位为 0, 则不命中。此时, 将 0240CH 单元所在的主存第 0000 001 0010 块 (即第 18 块) 复制到第 010 组 (即第 2 组) 的任意一个空闲行中, 并置有效位为 1, 置标记为 0000 0010 (表示数据来自第 2 组群)。
      ![[image-179.png]]
4.  **三种映射方式比较**
    *   **关联度 (Associativity)**：指一个主存块映射到cache中时可能存放的位置个数。
        *   直接映射：关联度为1 (最低)。
        *   全相联映射：关联度为cache总行数 (最高)。
        *   N路组相联映射：关联度为N。
    *   **命中率**：关联度越高，命中率通常越高（冲突越少）。
    *   **命中时间**：关联度越低，判断是否命中的开销越小，命中时间越短。
    *   **标记存储开销**：关联度越低，标记所占额外空间开销越少。

**例4.9：不同关联度下标记总位数计算**
    假定主存地址为 32 位, 按字节编址, 主存块大小为 16 字节, cache 最多能存放 4K 个主存块数据, 则在关联度分别为 1、2、4 和全相联方式下标记所占总位数是多少?
    比较在关联度为1、2、4和全相联方式下标记所占总位数。
      **解:**  
      关联度为 1 (直接映射) 时, 每组 1 行, 共 4K 组, 标记占 32-4-12=16 位, 总位数占 4K×16=64K 位。  
      关联度为 2 (2-路组相联) 时, 每组 2 行, 共 2K 组, 标记占 32-4-11=17 位, 总位数占 4K×17=68K 位。  
      关联度为 4 (4-路组相联) 时, 每组 4 行, 共 1K 组, 标记占 32-4-10=18 位, 总位数占 4K×18=72K 位。  
     全相联时, 整个为一组, 每组 4K 行, 标记占 32-4=28 位, 总位数占 4K×28=112K 位。

**4.6.4 cache中主存块的替换算法**

当一个新的主存块需要复制到cache，而cache中对应的行（或组）已经全部被占满时，必须选择淘汰掉一个cache行中的主存块，以便为新的主存块腾出空间。

常用的替换算法有：

1.  **先进先出算法 (FIFO - First-In-First-Out)**
	  ![[image-180.png]]
    *   **思想**：总是选择最早装入cache的主存块被替换掉。
    *   **实现**：实现简单，但不能正确反映程序的访问局部性（最早进入的块也可能目前仍被频繁使用）。
    *   **Belady异常**：可能存在增加cache组的行数反而导致命中率下降的现象。
 
2.  **最近最少使用算法 (LRU - Least Recently Used)**
    *   ![[image-181.png]]
    * **思想**：总是选择近期最少使用的主存块被替换掉。
    *   **依据**：基于访问局部性原理，近期最少使用的块，将来被访问的可能性也较小。
    *   **实现**：比FIFO复杂，通常需要为每个cache行（或组内行）设置计数器或使用堆栈结构来记录使用情况。
    *   **优点**：能较好地反映程序访问局部性，命中率通常较高。是堆栈算法。
    *   **缺点**：当程序的分块局部化范围超过cache组大小时，命中率可能急剧下降（颠簸/抖动）。
    *   **实现方式示例 (图4.31)**：用计数器记录各行的使用情况。
	    * ![[image-182.png]]
        *   命中时：被访问行的计数器清0，比其低的计数器加1，其余不变。
        *   未命中且有空闲行：新装入行的计数器设为0，其余全加1。
        *   未命中且无空闲行：计数值最大的行被替换，新装入行的计数器设为0，其余加1。
    *   **近似LRU**：由于硬件实现复杂，常采用近似LRU算法。

3.  **最不经常使用算法 (LFU - Least Frequently Used)**
    *   **思想**：替换掉cache中引用次数最少的块。
    *   **实现**：也用与每个行相关的计数器来实现。
    *   **与LRU的区别**：LFU基于访问频率，LRU基于最近访问时间。
4.  **随机替换算法 (Random)**
    *   **思想**：从候选行的主存块中随机选取一个淘汰掉，与使用情况无关。
    *   **优点**：实现简单，代价低。
    *   **性能**：模拟试验表明，随机替换算法在性能上只稍逊于基于使用情况的算法。

**例4.10：LRU替换算法分析与速度提高计算**
	假定主存空间大小为 32K×16 位, 按字编址, 每字 16 位。cache 采用 4-路组相联映射方式, 数据区大小为 4K 字, 主存块大小为 64 字。假定 cache 开始为空, 处理器按顺序访问主存单元 0, 1, …, 4351, 一共重复访问 10 次。假设 cache 比主存快 10 倍, 采用 LRU 替换算法。试分析采用 cache 后速度提高了多少?
    分析在特定访问序列下，4路组相联cache使用LRU替换算法的命中情况，并计算采用cache后的速度提高倍数。
	**解答**
     ![[image-183.png]]

**4.6.5 cache的一致性问题**
由于cache中的内容是主存块的副本，当对cache中的内容进行更新时，就存在cache和主存如何保持一致的问题。此外，当多个设备（如DMA控制器、多CPU）共享主存时，也可能出现cache一致性问题。
主要关注写操作：

5.  **全写法 (Write-Through / Store-Through)**
    *   **基本做法**：写操作时，若写命中，则同时写cache和主存。
    *   **写不命中时处理方式**：
        *   **写分配法 (Write Allocate)**：先在主存块中更新相应单元，然后分配一个cache行，将更新后的主存块装入该cache行。能利用空间局部性，但每次写不命中都需读主存块，开销大。
        *   **非写分配法 (Not Write Allocate / Write Around)**：仅更新主存单元，不装入主存块到cache。减少读主存块时间，但未利用空间局部性。
    *   **也称**：通写法、直写法、写直达法。
    *   **优点**：cache和主存数据始终一致，替换时无需写回。
    *   **缺点**：写操作开销大（每次都要写主存）。
    *   **优化**：通常在cache和主存之间加一个**写缓冲 (Write Buffer)**。CPU写cache的同时将信息写入写缓冲，然后由存储控制器将缓冲内容写主存。写缓冲是一个FIFO队列。

6.  **回写法 (Write-Back / Copy-Back)**
    *   **基本做法**：CPU执行写操作时，若写命中，则信息只被写入cache而不写入主存。
    *   **写不命中时**：通常采用写分配法，将主存块调入cache行并更新相应单元。
    *   **修改位 (Dirty Bit / Modified Bit)**：每个cache行设置一个修改位。若该行被修改过，则修改位置1。
    *   **替换时**：只有当cache行中的主存块被替换时，才检查其修改位。若修改位为1（脏块），则将该块内容一次性写回主存；若为0（干净块），则无需写回。
    *   **也称**：一次性写方式、写回法。
    *   **优点**：减少了写主存的次数，降低了主存带宽需求。
    *   **缺点**：cache和主存内容可能不一致，增加了控制复杂性（需要处理一致性协议）。

**4.6.6 cache性能评估**
	计算机性能最直接的度量方式是CPU时间。
	`CPU时间 = (CPU执行时钟数 + cache缺失引起阻塞的时钟数) * 时钟周期`
	`cache缺失引起阻塞的时钟数 = 读操作阻塞时钟数 + 写操作阻塞时钟数`
	`读操作阻塞时钟数 = 程序中读操作次数 * 读缺失率 * 读缺失代价`
	对于写操作的阻塞时钟数，根据不同的写策略，计算方式不同：
*   **回写法**：`写操作阻塞时钟数 = 程序中写操作次数 * 写缺失率 * 写缺失代价 + 写回阻塞`
*   **全写法**：`写操作阻塞时钟数 = 程序中写操作次数 * 写缺失率 * 写缺失代价 + 写缓冲阻塞`
若忽略写回阻塞和写缓冲阻塞，则可简化为：
`cache缺失引起阻塞的时钟数 = 程序的访存次数 * 缺失率 * 缺失代价`   
                     `= 程序的指令条数 * (缺失数 / 指令) * 缺失代价`

**例4.11：cache性能分析 (分离cache)**
假设某计算机中只有一级 cache, 并将指令和数据分别存放在 code cache 和 data cache 中。其 Code Cache 和 Data Cache 的缺失率分别为 1% 和 4%。假定在没有任何访存阻塞时的 CPI 为 1, 缺失损失为 200 个时钟周期。假定访存指令 (load 和 store) 的使用频度为 36%, 则使用缺失率为 0 的 cache 时, 处理器速度会快多少?
分析指令cache和数据cache分别存在时的CPU性能影响。
*   计算访问指令和访问数据的阻塞时钟数，两者相加得到总阻塞时钟数。
*   计算由于访存阻塞导致的CPI增加量。
*   计算处理器速度提升倍数（与无cache或无缺失cache比较）。
     ![[image-184.png]]
**进一步分析**：
*   **CPI越小，cache缺失引起的阻塞对系统总体性能的影响越大。**
*   **CPU时钟频率越高，cache缺失代价（以时钟周期计）就越大，对性能影响也越大。**
**\*4.6.7 影响cache性能的因素**
除了命中率（与关联度、容量、块大小、替换算法等有关），还有其他因素影响cache性能：
1.  **单级/多级 cache、联合/分离 cache的选择问题**
    *   **单级cache**：早期只有一级CPU片内cache。
    *   **多级cache**：现代CPU普遍采用多级cache（如L1, L2, 甚至L3）。
        *   L1 cache：通常在CPU芯片内，速度最快，容量最小。常采用分离cache（指令cache和数据cache分开）。
        *   L2, L3 cache：可以在CPU芯片内或外。通常为联合cache（指令和数据混合存放）。
    *   **访问流程**：CPU访问L1 cache缺失时，先从L2 cache找；L2缺失再从L3找（如果有）；最后才访问主存。
    *   **缺失率概念**：
        *   **全局缺失率**：在所有级别cache中都缺失的访问次数 / 总访问次数。
        *   **局部缺失率**：在某级cache中缺失的访问次数 / 对该级cache的总访问次数。
    *   **设计目标**：
        *   L1 cache：更关注速度（低命中时间）。
        *   L2/L3 cache：更关注提高命中率（降低全局缺失率）。

**例4.12：增加L2 cache对性能的提升**
    假定某处理器的时钟频率为 1.2GHz, 当 L1 cache 无缺失时的 CPI 为 1。访问一次主存的时间为 100ns (包括所有缺失处理), L1 cache 的局部缺失率为 2%。若增加一个 L2 cache, 并假定 L2 cache 的访问时间为 5ns, 而且其容量足够大到使全局缺失率仅为 0.5%, 问: 增加 L2 cache 后处理器执行程序的效率提高了多少?
    比较只有L1 cache和同时有L1、L2 cache时的CPI，计算效率提高倍数。
     **解:**  
	若仅有 **L1 cache**, 则仅发生 L1 cache 缺失, 其缺失损失为 100ns × 1.2GHz = 120 个时钟周期; 此时, 由于访存阻塞而使得 CPI 从 1 变为 1+120×2% = 3.4。  
	若同时又有 L2 cache, 则存在以下两种情况。  
	(1) 若 $L1 cache$ 缺失而 $L2 cache$ 命中, 则**缺失损失**为 5ns × 1.2GHz = 6 个时钟周期。  
	(2) 若 L1 和 L2 cache 都缺失, 则需访问主存, 缺失损失为 100ns × 1.2GHz = 120 个时钟周期。  
	因此, 由于**访存阻塞**而使得 CPI 数从 1 变为 1+6×(2%-0.5%)+120×0.5% = 1.69。  
	综上可知, 增加 **L2 cache** 使处理器执行程序效率提高了 3.4/1.69 ≈ 2 倍。  
	由于多级 cache 中各级 cache 所处的位置不同, 使得对它们的设计目标有所不同。例如, 假定是两级 cache, 那么, 对于 **L1 cache**, 通常更关注速度而不要求有很高的命中率, 因为, 即使不命中, 还可以到 L2 cache 中访问, **L2 cache** 的速度比主存速度快得多; 而对于 L2 cache, 则要求尽量提高其命中率, 因为若不命中, 则必须到慢速的主存中访问, 其缺失损失会很大。

2.  **主存—总线—cache间的连接结构问题**
    *   **目标**：使主存块在主存和cache之间的传输速度最快，以减小缺失代价。
    *   **传输过程三阶段**：
        1.  发送地址和读命令到主存。
        2.  主存准备好一个数据。
        3.  从总线传送一个数据。
    *   **连接方式对缺失代价的影响 (假设主存块有4个字)**：
	    * ![[image-185.png]]
        *   **窄形结构**：主存、总线、cache间每次按一个字的宽度传送。缺失代价 = 4 * (发地址+准备数据+传数据)。 (图4.33a)
        *   **宽形结构**：主存、总线、cache间每次按多个字的宽度传送。若宽度为2个字，缺失代价 = 2 * (发地址+准备数据+传数据)。若宽度为4个字，缺失代价 = 1 * (发地址+准备数据+传数据)。 (图4.33b)
        *   **交叉存储器结构**：主存采用多模块交叉存取，总线和cache间每次按一个字的宽度传送，但可流水线式启动各模块。缺失代价 = 1 * (发地址) + 1 * (首字准备时间) + 4 * (传数据时间)。 (图4.33c)
    *   **结论**：交叉存储器结构的性价比通常最好。
3.  **DRAM结构、总线事务类型与cache的配合问题**
    *   **背景**：主存由DRAM芯片实现，cache缺失时需从DRAM读取信息块到cache。
    *   **DRAM结构优化**：如DRAM内部的行缓冲 (SRAM)，当CPU访问一块连续的主存区（行地址相同）时，可直接从行缓冲读取，速度快。
    *   **总线事务类型**：如突发传输 (Burst Transfer)，cache缺失时，给定首地址后，可连续高效地传输整个主存块。
    *   **DDR SDRAM技术**：芯片内部交叉多数据预取，总线时钟上升沿和下降沿均传送数据，提高数据块传送效率。

**\*4.6.8 cache结构举例 (Pentium 和 Pentium 4)**

*   **Pentium**：
    *   片内集成代码cache和数据cache (分离L1 cache)。
    *   数据cache采用双端口结构，支持两个整数ALU并行访问。
    *   两路组相联，每路4KB，总共8KB。每组2行，每行32字节（8个双字）。
    *   目录表：每路一个，各128项，每项包含20位标记和2位状态（MESI协议）。
    *   LRU替换策略，每组一个LRU位。
    *   数据cache采用回写策略，也可动态重构为全写法。
    *   支持片外二级cache (L2 cache)。

*   **Pentium 4**：
    *   片内集成L2 cache和两个L1 cache (L1数据cache, L1指令cache - Trace Cache)。
    *   L2 cache：联合cache，数据和指令混合存放，所有从主存获取的指令和数据先送到L2。
    *   L2 cache对外有一个端口与处理器总线相连，对内有两个端口分别与L1数据cache和指令预取部件相连。
    *   L1指令cache (Trace Cache, TC)：存放的不是指令，而是指令对应的微操作序列。

